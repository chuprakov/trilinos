Users' guide to MpiTypeTraits

* Summary

This class maps from the C++ type, to its ~MPI_Datatype~ and count.  For a given C++ type ~Packet~, this traits class tells you:
1. The corresponding ~MPI_Datatype~ and count (i.e., number of contiguous instances of that ~MPI_Datatype~).  These tell MPI how to send and receive objects of type ~Packet~.
2. Whether the ~MPI_Datatype~ is a /basic/ or /derived/ type.  Derived types must be freed after use, by calling ~MPI_Type_free~.  Basic types need not and must not be freed after use.
3. Whether the ~MPI_Datatype~ and count are the same for all instances of ~Packet~ on an MPI process, at all times in the program, on all MPI processes.  (These are three separate conditions.)  
4. Whether you need to serialize a ~Packet~ instance before communicating it.  (Serialization is handled separately.)

~MpiTypeTraits~ is meant mainly for Trilinos packages such as Tpetra and Zoltan2, that use Comm (Teuchos' MPI wrapper) to communicate data.  It is also appropriate for any direct user of Comm.  If you do not use Comm directly, you need not worry about this class.

This class works by specialization.  If there is no specialization of ~MpiTypeTraits~ for a type ~Packet~, then expressions involving ~MpiTypeTraits<Packet>~ will not compile.  We provide specializations for many commonly used C++ types, including all standard built-in integer and floating-point types.  However, you must write a specialization for any ~Packet~ types we do not cover.  Be sure to check this header file for the list of specializations that we provide.  Duplicating an existing specialization will result in an error either at compile time or link time.

* When and how do I need to serialize?

In this section, we explain when you need to serialize instances of a particular type ~Packet~ before communicating them.  We also talk about how to use MPI's built-in serialization capability, and how to do serialization if not building with MPI.  Novice users should read the initial "Summary" section.  Subsequent sections include justification for design choices, and implementation suggestions.

** Summary: What user code needs to do

You must serialize when communicating
- An array of indirect type ~T~
- An array of arrays of any type ~T~
- An array of ~T~, if ~T~ may have count greater than one
- An instance of ~T~, if ~MpiTypeTraits<T>::mustSerialize~ is true

The Boolean ~MpiTypeTraits<T>::direct~ is true if ~T~ is a direct type, and false if ~T~ is an indirect type.  An /direct type/ ~T~ is a type for which it is correct to use a pointer to a ~T~ instance as an input or output buffer of an MPI function.  For example:
#+BEGIN_SRC C++
T x (...); // T here must be a direct type.
(void) MPI_Send (&x, ...);
#+END_SRC
An /indirect type/ is a type for which this is not the case.  Some indirect types require serialization in order to communicate a single instance; others do not.  For example, ~std::string~ is an indirect type, but does not require serialization.  This is because for a ~std::string~ instance ~s~, one may use ~reinterpret_cast<void*> (s.c_str ())~ as an input or output buffer.  In contrast, ~std::set<T>~ for any ~T~ is both indirect and requires serialization.  This is because the representation of ~std::set~ is opaque, /and/ an ~std::set~ need not store its data contiguously (as ~std::string~ and ~std::vector~ must).  That is, there is no sensible way to define "get the raw pointer" for an ~std::set~.

The Boolean ~MpiTypeTraits<T>::mustSerialize~ is true if ~T~ requires serialization.  For types ~T~ that do /not/ require serialization, ~MpiTypeTraits<T>::getPtr~ returns a pointer that may be used as an input or output buffer.  Here is an example of a send for a type that does not need serialization.
#+BEGIN_SRC C++
std::string s ("foo bar");
void* buf = MpiTypeTraits<T>::getPtr (s);
int count = MpiTypeTraits<T>::getCount (s);
MPI_Datatype type = MpiTypeTraits<T>::getType (s);
(void) MPI_Send (buf, count, type, destRank, tag, comm);
#+END_SRC
For types that /do/ require serialization, ~MpiTypeTraits<T>::getPtr~ is not needed.  It just returns the address of the ~T~ instance in that case.  Here is an example of a send for a type ~T~ that requires serialization.  ~SerializationTraits<T>::pack~ is a hypothetical serialization traits class in this example, that uses ~MPI_Pack~ internally.
#+BEGIN_SRC C++
T x (...);
int count = MpiTypeTraits<T>::getCount (s);
char* buf = new char [count];
// Serialize x into the char buffer.  The MPI_Comm 
// argument 'comm' is needed because MPI_Pack needs it.
SerializationTraits<T>::pack (buf, x, comm);
// Data serialized by MPI always have type MPI_PACKED.
MPI_Datatype type = MPI_PACKED;
(void) MPI_Send (buf, count, type, destRank, tag, comm);
#+END_SRC
Note that it's also legal to serialize types ~T~ that themselves do not need serialization.  This is necessary when sending an array of arrays of ~T~.  In that case, one should be careful to use ~MPI_PACKED~ as the ~MPI_Datatype~, not the datatype of ~T~.

** Getting a valid buffer argument for MPI functions

The ~std::string~ type is an indirect type, which nevertheless just represents a simple array of characters.  Communicating a string should not require serialization, as long as the receiving process knows the number of characters.  This is also true of types more important to numerical computation, like ~std::vector<double>~ or other encapsulations of arrays.  This suggests that ~MpiTypeTraits~ should provide a generic way to get "a pointer to the data," even when simply taking the address of the ~Packet~ instance is not correct.  The MPP library does this by providing a ~get_ptr~ class method in its traits class analogous to ~MpiTypeTraits~.  This method returns a pointer which can be given directly to MPI as an input or output buffer.  Our ~MpiTypeTraits~ traits class has a corresponding class method ~getPtr~.  It looks like this:
#+BEGIN_SRC C++
/// \brief Pointer to give to MPI as an input or output buffer.
///
/// If mustSerialize is false (that is, if Packet need not be serialized), 
/// then you may pass the pointer returned by this function directly to 
/// MPI as an input or output buffer for communicating a Packet instance.  
///
/// This function returns <tt>void*</tt> (not even <tt>const void*</tt>) 
/// because MPI's C binding does not know about \c const or about the 
/// Packet type.  Both input and output buffers in MPI are <tt>void*</tt>.
static void* getPtr (const Packet& x);
#+END_SRC
The canonical example is ~std::string~, but this works if ~Packet~ is any kind of container type for which MPI can't apply the ~MPI_Datatype~ directly to the address of a ~Packet~ instance.  For example, the ~std::string~ version looks like this:
#+BEGIN_SRC C++
static void* getPtr (const std::string& x) {
  return reinterpret_cast<void*> (const_cast<char*> (s.c_str ()));
}
#+END_SRC
and the ~std::vector<T>~ partial specialization might look like this (though note that arrays of indirect types are not valid MPI buffers; see below):
#+BEGIN_SRC C++
static void* getPtr (const std::vector<T>& x) {
  if (x.size () == 0) {
    return NULL; // Don't do &x[0] if x[0] doesn't exist.
  } else {
    return reinterpret_cast<void*> (const_cast<T*> (&x[0]));
  }
}
#+END_SRC

** Arrays of indirect types are not valid MPI buffers

Using ~getPtr~ for an indirect type make sense if you are sending or receiving just one ~Packet~.  What about an array of ~Packet~, though?  MPI's datatype system for an /array/ of data reads the address of each element of the array directly.  If you give it a ~Packet packets[]~ array, it will apply the ~MPI_Datatype~ to ~&packets[i]~ for each element ~packets[i]~ of the array.  This makes sense for an array of ~double~ or even an array of certain kinds of struct-like objects, like ~std::complex<double>~.  However, it doesn't make sense if ~Packet~ contains dynamically allocated data or if it has an opaque layout -- that is, if ~Packet~ is an indirect type.  For example, neither you nor MPI know the internal representation of an ~std::string~.  Some ~std::string~ implementations use reference counting to avoid storing duplicates redundantly.  What MPI needs is ~getPtr (strings[i])~, not ~&strings[i]~, but MPI doesn't know that it must call ~getPtr~ on each element of the array.  

** Arrays of indirect types need serialization; arrays of direct types do not

We solve this problem by requiring serialization for communicating arrays of indirect ~Packet~ types.  Note that a single instance of an indirect type does /not/ need serialization.  The array type ~std::vector<double>~ is an indirect type, for example, but it is easy to send or receive its data directly.  Requiring serialization in this case would tempt users to bypass useful array abstractions (like ~std::vector~ or Teuchos' memory management classes) in favor of raw pointers.  This is the justifiation for distinguishing between direct and indirect types.

Indirect types include ~std::string~, ~std::vector<T>~ for any ~T~, and ~Teuchos::ArrayView<T>~ for any ~T~.  An indirect type which does /not/ require serialization may be sent or received directly; the pointer returned by ~getPtr~ may be used directly as an input or output buffer in an MPI function.  Thus: 
- ~double~ can be sent directly
- ~std::vector<double>~ can be sent directly
- ~std::string~ can be sent directly, but ~std::vector<std::string>~ cannot
- ~std::vector<T>~ can only be sent directly if ~T~ is a direct type

This implies that every indirect type must have a definition of serialization and deserialization.  It would also be reasonable to require that every direct type have a count of 1.  For example, ~qd_real~ would use a custom ~MPI_Datatype~ (made using ~MPI_Type_contiguous~) with a count of 1, rather than ~MPI_DOUBLE~ with a count of 4.  This makes computing the count of an array of ~Packet~ much easier, and also avoids possible errors relating to alignment of structs (for example, if you cheat by using ~MPI_Type_contiguous~ for a struct of three doubles).  As we explain elsewhere, this introduces no additional restrictions on one-sided communication (types like ~qd_real~ need a custom ~MPI_Op~ for addition anyway).  

** Array-like types

We define /array-like/ types as those with a count greater than 1.  Intuitively, these are types which from MPI's perspective represent "multiple values" to communicate.  Whether a type is array-like has consequences for serialization, as we explain below.  We prefer that types which represent "single values," rather than an array of values, always have a count of 1.  For example, there are two different ways to represent a ~qd_real~: 
1. ~MPI_Datatype~ of ~MPI_DOUBLE~ and count of 4
2. a custom ~MPI_Datatype~ (via ~MPI_Type_contiguous~) and a count of 1
Always choosing a custom ~MPI_Datatype~ with a count of 1 has advantages.  First, it makes computing the count of an array of ~Packet~ easier.  The user need not remember to multiply the count of an element by the number of elements in the array.  Second, it avoids possible errors relating to alignment of structs.  For example, for a direct type ~T~ that looks like a struct of three ~char~, it might be tempting to use a datatype of ~MPI_CHAR~ and a count of 3.  However, the compiler [[http://en.wikipedia.org/wiki/Data_structure_alignment#Data_structure_padding][is allowed to pad the end of a struct]] with unused data, in order to preserve a particular alignment requirement (e.g., 4-byte alignment, which in this case would make each ~T~ take up 4 bytes).  If you have an array of ~T~ with 10 elements, and use a datatype of ~MPI_CHAR~ and a count of 3 for each element, it would be tempting to represent the array with a datatype of ~MPI_CHAR~ and a count of 30.  This is wrong, though, if ~T~ is aligned to 4 bytes.  Using a custom datatype and a count of 1 for each ~T~ instance would fix this problem, whether you use ~MPI_Type_struct~ or ~MPI_Type_contiguous~ to build the custom datatype.  Third, using a count of 1 avoids serialization.  As we explain below, array-like types of array-like types need serialization.  For the above "array of ~T~" example, it would be correct (but slow) to serialize the whole array.  This is unnecessary, though, because ~T~ is a direct type.  Using a count of 1 for ~T~ makes it clear that MPI can legally send and receive the array of ~T~ directly.

Using a custom datatype for "single values" introduces no additional restrictions on one-sided communication.  This is because nearly any C++ types not built into the language needs a custom ~MPI_Op~ for reductions anyway, and one-sided communication only works for built-in ~MPI_Op~, not custom ones.  The type ~qd_real~ is an example.

** Array-like types of array-like types need serialization

Sending an array of an array-like type always requires serialization or some form of packing.  We've explained this already above in the discussion of direct vs. indirect types, but the issue is even more fundamental than that.  Suppose we have a length $N$ array of arrays, each of which has a possibly different length $M_i$.  There is no guarantee that we can send and receive this as a single contiguous array of length $N \cdot \sum_i M_i$, since the arrays need not be stored contiguously.  This is true even if all the arrays have the same length $M$.  If we make a custom ~MPI_Datatype~ for each array, then different arrays have different datatypes.  MPI requires, however, that all elements of an input or output buffer array have the same ~MPI_Datatype~.  

** Everything must define serialization, even if it doesn't need it

An array of indirect types needs serialization.  This implies that every indirect type ~T~ needs to have a serialization procedure defined, even if communicating a single ~T~ instance does not require serialization.  Furthermore, an array of arrays (or array-like types) of ~T~ needs serialization, even if ~T~ is a direct type.  This means that even direct types must have a serialization procedure defined.  /Every/ ~Packet~ type needs to define ~pack~ and ~unpack~ methods somewhere.

The nice thing is that MPI already provides serialization, via ~MPI_Pack~ and ~MPI_Unpack~.  These functions use the ~MPI_Datatype~ to describe how to serialize and deserialize the data.  Thus, if ~MpiTypeTraits<T>::mustSerialize~ is false, we may defer serialization to MPI in a generic way:
#+BEGIN_SRC C++
Teuchos::ArrayView<char>::size_type 
pack (Teuchos::ArrayView<char> buf, const Packet& x, MPI_Comm comm) 
{
  void* inPtr = MpiTypeTraits<T>::getPtr (x);
  int inCount = MpiTypeTraits<T>::getCount (x);
  MPI_Datatype inType = MpiTypeTraits<T>::getType (x);

  char* outPtr = buf.getRawPtr ();
  int outSize = static_cast<int> (buf.size ());
  int pos = 0;
  (void) MPI_Pack (inPtr, inCount, inType, outPtr, outSize, &pos, comm);

  // Return the starting position for the next item, if there
  // is one.  MPI lets you pack multiple items of possibly 
  // different types in the same buffer.
  return static_cast<Teuchos::ArrayView<char>::size_type> (pos);
}
#+END_SRC
and likewise deserialization:
#+BEGIN_SRC C++
Teuchos::ArrayView<char>::size_type 
unpack (Packet& x, Teuchos::ArrayView<char> buf, MPI_Comm comm) 
{
  void* outPtr = MpiTypeTraits<T>::getPtr (x);
  int outCount = MpiTypeTraits<T>::getCount (x);
  MPI_Datatype outType = MpiTypeTraits<T>::getType (x);

  char* inPtr = const_cast<char*> (buf.getRawPtr ());
  int inSize = static_cast<int> (buf.size ());
  int pos = 0;
  (void) MPI_Unpack (inPtr, inSize, &pos, outPtr, outCount, outType, comm);

  // Return the starting position for the next item, if there
  // is one.  MPI lets you unpack multiple items of possibly 
  // different types from the same buffer.
  return static_cast<Teuchos::ArrayView<char>::size_type> (pos);
}
#+END_SRC
If ~Packet~ needs serialization anyway, then ~pack~ and ~unpack~ need custom definitions.

** Why is serialization separate from ~MpiTypeTraits~?

Separating the implementation of serialization from ~MpiTypeTraits~ lets us change the serialization method, without changing how we communicate the serialized data.  That way, we can build with or without MPI, just by changing the serialization traits class.  (See the example in the next subsection.)  Users could also write their own serialization routines.  If you want to be clever, you could let the serialization traits class define the ~MPI_Datatype~ or even the buffer type (not necessarily ~char~) of packed data.  The right way to do this would be to use a different serialization traits class.  Here is a hypothetical example, with a different serialization traits class ~FancySerializationTraits~.  It's fair to assume that all instances of the buffer type have the same basic ~MPI_Datatype~ and the same count of 1.
#+BEGIN_SRC C++
T x (...);
int count = MpiTypeTraits<T>::getCount (s);
typedef typename FancySerializationTraits<T>::pack_type pack_type;
pack_type* buf = new pack_type [count];
// Somehow serialize x into the buffer.
FancySerializationTraits<T>::pack (buf, x, comm);
// In this case, serialized data need not have type MPI_PACKED.
// We assume MpiTypeTraits<pack_type>::getCount always returns 1.
MPI_Datatype type = MpiTypeTraits<pack_type>::getType (pack_type ());
(void) MPI_Send (buf, count, type, destRank, tag, comm);
#+END_SRC
For example, given an array of arrays of ~double~ with no ~NaN~ entries, one might like to pack them into a single array of ~double~, with ~NaN~ delimiters between each packed array.  In that case, ~FancySerializationTraits<double>::pack_type~ would be ~double~.  (I don't necessarily recommend this; I'm merely coming up with an example.)

** How do I define serialization if I'm not building with MPI?

Trilinos has a requirement that key packages can build without MPI.  This is an annoying and troublesome requirement, because it forces us to reimplement much of MPI, and hope that we got the semantics right.  Nevertheless, it is a requirement.

The good thing is that when running without MPI, there is only one "process."  The size of ~MPI_COMM_WORLD~ is 1.  This takes away all worries about type punning and endianness.  For example, I can serialize by copying a double directly into an array of 8 ~char~ using ~memcpy~:
#+BEGIN_SRC C++
template<>
NoMpiSerializationTraits<double>::
pack (Teuchos::ArrayView<char> buf, const double& x, FakeComm) {
  // size_type is signed; size_t is unsigned.
  typedef Teuchos::ArrayView<char>::size_type size_type; 
  const size_type size = static_cast<size_type> (sizeof (double));
  if (buf.size () < size) {
    throw std::invalid_argument ("buffer is not long enough");
  } else { // memcpy wants a size_t, not a size_type.
    memcpy (&x, buf.getRawPtr (), sizeof (double));
    return size;
  }
};
#+END_SRC
We use ~memcpy~ because ~reinterpret_cast<char*> (&x)~ does not necessarily work correctly, due to possible compiler optimizations.

The problem with this approach is that now we have to reimplement ~MPI_Pack~ and ~MPI_Unpack~.  We must do this for /every/ type, not just those that require serialization.  Furthermore, we must do this without the benefit of the MPI datatype system, which describes the memory layout of many different kinds of data.  At least the above "~memcpy~ ~sizeof(T)~ bytes" approach works for all C++ built-in types and many other types, such as ~std::complex<double>~.

* When and how do I do count coordination?
** Summary: What user code needs to do

There are three Booleans in ~MpiTypeTraits<T>~ that tell you exactly whether and how to do count coordination with instances of ~T~.
- ~sameDatatype~: If true, then all instances of ~T~, on all processes, at all times, have the same ~MPI_Datatype~.
- ~sameLocalCount~: If true, then all instances of ~T~ on the calling process, at all times, have the same count.
- ~sameGlobalCount~: If true, then all instances of ~T~, on all processes, at all times, have the same count.

If ~sameDatatype~ is false, then there is no generic way to do count coordination.  An example of this case would be if ~T~ is an encapsulation of a slice of a two-dimensional structured grid.  The sending and receiving process might use an entirely different custom ~MPI_Datatype~ to handle different strides.  Users must do coordination themselves in a way specific to the type ~T~.

If ~sameDatatype~ is true, then users can do count coordination by sending the count, since the sending and receiving process use the same ~MPI_Datatype~.  There are three cases, depending on the other two Booleans:

1. ~sameLocalCount~ is false: Sending a single ~T~ instance always requires knowing the count for that instance.  One can't just use any instance to determine the count.  Preallocating enough buffer space for serializing an array of ~T~ may require examining all the elements of the array, not just the first.  It may be faster to use a dynamic reallocation strategy, with a reasonable upper bound.
2. ~sameLocalCount~ is true, but ~sameGlobalCount~ is false: The count for a ~T~ instance is a "per-process" value.  It's still necessary to coordinate on the count.  Preallocating enough buffer space to serialize an array of ~T~ does not require checking the count of every element of the array.
3. ~sameLocalCount~ is true, and ~sameGlobalCount~ is true: All instances of ~T~ everywhere have the same count at all times.  There is no need to do count coordination, whether or not serialization is necessary.  

Cases 2 and 3 impose a constraint on types ~T~ that do not require serialization:  If all instances of ~T~ have the same count, then this is also true of their serializations.  

The ~sameLocalCount~ parameter does not affect serialization for an array of ~T~, as long as the serialization of a ~T~ instance encodes the amount of data somehow.  Sending an array of ~T~ when ~sameGlobalCount~ is false always requires serialization and coordination on the total amount of serialized data.

Note that the count coordination requirement does not mean that users must necessarily send the count each time.  Users may make assertions about ~sameDatatype~, ~sameLocalCount~, or ~sameGlobalCount~ at run time for a particular type ~T~.

