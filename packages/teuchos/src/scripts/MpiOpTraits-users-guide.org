MpiOpTraits Users' Guide

* Design issues
** Reduction operators

Certain parallel operations, such as reductions and scans, combine data using an arbitrary associative function.  We call this function the /reduction operator/, whether we are performing a reduction, a scan, or some other related operation.  MPI encapsulates a reduction operator using the ~MPI_Op~ opaque handle type.  Analogously to ~MPI_Datatype~, MPI comes with a set of predefined reduction operators.  It also lets users define their own by specifying an arbitrary function of a certain signature[fn:predefined-op].  Users are responsible for ensuring that the function is associative.  They may also assert its commutativity, which lets MPI make optimizations.  MPI cannot check commutativity or associativity itself[fn:custom-op-source].

[fn:predefined-op] See Section 5.9.2 of the MPI 3.0 Standard for a complete list of predefined reduction operators.

[fn:custom-op-source]  MPI cannot examine the source code of the user's function.  Decompiling arbitrary compiled code is hard and may even violate certain license agreements.  Even if MPI could do this, proving commutativity or associativity from source code would be unreasonably difficult.

** MPI treats predefined and custom operators differently

~MPI_Op~ also resembles ~MPI_Datatype~ in that MPI does not treat custom reduction operators the same way it treats predefined ones.  Predefined operators can be used in any MPI function that takes an ~MPI_Op~.  This includes applicable one-sided communication operations, such as ~MPI_Accumulate~.  What a predefined operator does depends on the ~MPI_Datatype~ as well as the ~MPI_Op~.  For example, ~MPI_SUM~ sums two ~double~ values (~sizeof(double) == 8~) if the datatype is ~MPI_DOUBLE~, but two ~int~ values (~sizeof(int) == 4~) if the datatype is ~MPI_INT~.  Predefined operations only work with predefined ~MPI_Datatype~; MPI does not know how to sum or take the min of two values of arbitrary datatype.  Custom reduction operators, by contrast, do not work with one-sided communication.  Furthermore, MPI implementations may optimize collectives with predefined operators and datatypes.  For example, some network hardware may offload reductions from the CPU entirely in this case, so that the network adapters do all the computation as well as handling communication.  MPI implementations cannot accelerate custom reduction operators, since they have no idea what the user-specified function does inside.

This distinction between predefined and custom reduction operators affects both performance and correctness.  Thus, a C++ interface to MPI needs to expose it.  Users will want to write /generic/ code templated on the type ~Packet~ of data being communicated.  When writing generic code, users need to know at the time of constructing an operator whether it is predefined or custom.  Ideally, they would like to know this at compile time, but they should at least know at run time.  This lets them pick the correct communication path: 
- (possibly faster) one-sided communication for types with predefined reduction operators, or 
- two-sided communication for types with custom reduction operators.  
If our interface just took an arbitrary function pointer, then every ~MPI_Op~ it returned would be custom, and users could only use two-sided communication.

* ~MpiOpTraits~ traits class: first pass

This suggests a traits class approach, templated on the function type ~OpType~.  Here is the first-pass design for the ~MpiOpTraits~ traits class.
#+BEGIN_SRC C++
template<class OpType>
class MpiOpTraits {
public:
  typedef OpType op_type;

  /// \brief Whether MPI_Op_free must be called on the MPI_Op after use.
  ///
  /// If this is true, then you may not use the MPI_Op in one-sided communication.
  static const bool mustFree = true;

  /// \brief The MPI_Op, wrapped for automatic freeing after use.
  ///
  /// MPI_Op_free is called automatically after the reference count goes to zero.
  ///
  /// \param op [in] The reduction function to wrap.  
  /// \param commutative [in] If true, then MPI will assume that op is commutative.
  ///   MPI always assumes that op is associative.
  ///
  /// \note The ~commutative~ option cannot be a class Boolean, because 
  /// commutativity may not depend only on the type.  For example, given two 
  /// functions of the same type (that take the same arguments in the same
  /// order, and return the same type), one might commute and the other not.
  Teuchos::RCP<Teuchos::OpaqueWrapper<MPI_Op> > getOp (OpType op, const bool commutative=false);
};
#+END_SRC

** Definition for custom operators

It's easy to define ~MpiOpTraits~ for custom reduction operators.  (This neglects the important issue of serialization for certain kinds of input, which we discuss below.)  ~MPI_Op_create~ (which creates a custom operator) takes a function pointer, which MPI's C binding typedefs to ~MPI_User_function~.  This has the following signature:
#+BEGIN_SRC C++
typedef void (*MPI_User_function) (void* invec, void* inoutvec, int* len, MPI_Datatype* dtype);
#+END_SRC
We can then specialize ~MpiOpTraits~ for ~OpType=MPI_User_function~:
#+BEGIN_SRC C++
template<>
class MpiOpTraits<MPI_User_function> {
public:
  typedef MPI_User_function op_type;

  // Custom reduction operators must be freed after use.
  static const bool mustFree = true;

  // We show the implementation of freeCustomMpiOp below.
  RCP<OpaqueWrapper<MPI_Op> > getOp (op_type f, const bool commutative) {
    int commute = commutative ? 1 : 0;
    MPI_Op op = MPI_OP_NULL;
    const int err = MPI_Op_create (f, commute, &op);
    if (err != MPI_SUCCESS) {
      // ... handle error ... 
    } else { // Custom MPI_Op must be freed after use.
      return opaqueWrapper<MPI_Op> (op, freeCustomMpiOp);
    }
  }
};
#+END_SRC

** Definition for predefined operators

Users may access predefined operators by making the ~OpType~ template parameter a specific type, for instance, one of the templated function objects in the C++ Standard Template Library (STL).  We developers then must provide a specialization of ~MpiOpTraits~ for that function object type.  For example, ~OpType=std::plus<double>~ means ~getOp~ returns ~MPI_SUM~, with the expectation that you will use the correct ~MPI_Datatype~ (in this case, ~MPI_DOUBLE~).  Here is a specialization for ~OpType=std::plus<double>~:
#+BEGIN_SRC C++
template<>
class MpiOpTraits<std::plus<double> > {
public:
  typedef std::plus<double> op_type;

  // Predefined reduction operators must not be freed after use.
  static const bool mustFree = false;

  RCP<OpaqueWrapper<MPI_Op> > getOp (op_type f, const bool commutative) {
    (void) f; 
    (void) commutative;
    return opaqueWrapper<MPI_Op> (MPI_SUM);
  }
};
#+END_SRC
We may use automatic code generation to make writing specializations for predefined operators more efficient.

The STL provides some, but not all, function objects corresponding to predefined operations.  Here is a list of predefined operations in the MPI 3.0 Standard (Section 5.9.2), what they do, and the corresponding STL function object, if it exists.
- ~MPI_MAX~: maximum, ~std::max~
- ~MPI_MIN~: minimum, ~std::min~
- ~MPI_SUM~: sum, ~std::plus~
- ~MPI_PROD~: product, ~std::multiplies~
- ~MPI_LAND~: logical and
- ~MPI_BAND~: bit-wise and
- ~MPI_LOR~: logical or
- ~MPI_BOR~: bit-wise or
- ~MPI_LXOR~: logical exclusive or (xor)
- ~MPI_BXOR~: bit-wise exclusive or (xor)
- ~MPI_MAXLOC~: max value and location
- ~MPI_MINLOC~: min value and location
For operations that do not have a corresponding STL function object, we could define C++ classes where appropriate.  Users would need to use those classes in order to access predefined operations.

** How do custom ~MPI_Op~ returned by ~MpiOpTraits~ get freed?

Custom (user-defined, not predefined) ~MPI_Op~ must be freed after use by calling ~MPI_Op_free~.  Failing to do so may result in memory leaks (see e.g., [[https://software.sandia.gov/bugzilla/show_bug.cgi%3Fid%3D5724][Teuchos Bug 5724]]).  On the other hand, it is not legal to call ~MPI_Op_free~ after ~MPI_Finalize~ has been called.  This means that our "free" function used in the ~OpaqueWrapper~ has to check the latter before calling ~MPI_Op_free~.  We can do this with ~MPI_Finalized~:
#+BEGIN_SRC C++
void freeCustomMpiOp (MPI_Op op) {
  int finalized = 0;
  // It's not clear whether checking the error code would do much good.
  // You're in big trouble if MPI_Finalized doesn't work.  However, we 
  // conservatively free only if there was no error.  We don't throw an
  // exception on error, because this function would normally be called 
  // in a destructor.  Throwing an exception from a destructor is a bad
  // idea in general.
  const int err = MPI_Finalized (&finalized);
  if (finalized && err == MPI_SUCCESS) {
    // This sets op on output to MPI_OP_NULL.
    // Ignore the returned error code.
    (void) MPI_Op_free (&op);
  }
}
#+END_SRC

** How ~MpiOpTraits~ differs from ~MpiTypeTraits~

The ~MpiOpTraits~ traits class resembles ~MpiTypeTraits~, which maps from C++ type to its corresponding ~MPI_Datatype~.  However, reduction operations differ from C++ types in the following ways.  First, reduction operators are "only functions"; they may not hold "instance data."  Any data besides its input that the function needs must be class or global data.  The implication is that "any instance" of ~OpType~ may be passed into ~getOp~.  Thus, ~MpiOpTraits~ does not need the equivalent of ~MpiTypeTraits<T>::sameLocalCount~.  Second, MPI lets different processes supply different user-defined operators to the reduction (see Section 5.9.1 of the MPI 3.0 standard), although it recommends against this.  It's the user's responsibility to use the correct ~MPI_Op~ on each process.  As a result, ~MpiOpTraits~ does not need the equivalent of ~MpiTypeTraits<T>::sameDatatype~.  

* Deserialization and reserialization in custom operators

The above implementation for custom reduction operators neglects an important point: It is entirely legitimate to perform reductions on serialized data.  The data may have been serialized using ~MPI_Pack~, in which case the ~MPI_Datatype~ is ~MPI_PACKED~, or it may have been serialized by a custom method, in which case the ~MPI_Datatype~ could be any predefined datatype.  Thus, in order to operate on the data, the reduction operator has to deserialize the input and input-output arrays, compute the result, and reserialize the result to the output array.  

Ideally, users should not have to worry about serialization.  They should just be able to supply some binary function of a type ~T~, and let us handle serialization automatically if necessary for that type ~T~.  This would be easier if we could always use a custom reduction operator for every type ~T~, but as we mentioned above, it is important for performance and correctness reasons to use predefined operators whenever possible.  Furthermore, it may be necessary to consider this even when building without MPI.  If you choose to wrap ~MPI_Reduce_local~, then you actually do need to apply the reduction operator, even if there is only one process in the communicator.  Since building with or without MPI affects the choice of serialization method, the implementation must allow different serialization methods.

** Hiding serialization only works for certain types

If serialization is needed, then the reduction operator must be custom.  This means we can wrap the user-supplied function in our own function that handles serialization and deserialization.  (Teuchos::Comm already does this.)  In pseudocode, this would look like this:
1. Deserialize the input buffer ~invec~ into an array of ~const T~, and deserialize the input/output buffer ~inoutvec~ into an array of ~T~.
2. Call the user's function ~void (*f) (const T[], T[], int count)~.  
3. Reserialize the result back into the input/output buffer ~inoutvec~.
There is really no other way to automate the serialization process for user-supplied reduction functions.  This constrains how we can do serialization.  For example, MPI requires that our custom reduction have the following signature
#+BEGIN_SRC C++
typedef void (*MPI_User_function) (void* invec, void* inoutvec, int* len, MPI_Datatype* dtype);
#+END_SRC
This means that the amount of serialized data must be the same for the input buffer and for the input/output buffer.  Furthermore, for some types ~T~, the reduction operation might change the amount of data stored.  (Imagine a reduction on ~std::string~ that concatenates the two strings.)  Thus, the amount of serialized data must be an upper bound on the amount of data needed for each of the two buffers, and for the output.  What if users don't know the upper bound?  Furthermore, if we have automated serialization completely and hidden it from users, then we have to know the upper bound.  What if we don't?  The best we can do is guess, and abort if we got it wrong.  (It is legal to call ~MPI_Abort~ in a custom reduction operator (MPI 3.0 Standard, Section 5.9.5).)

** Supporting different serialization methods

The need to use possibly different serialization methods suggests the following options:
1. Template ~MpiOpTraits~ on the serialization traits class
2. Refactor serialization to use run-time polymorphism
3. Restrict serialization to two choices (MPI or not), and use ~#ifdef HAVE_MPI ... #else ... #endif~ in the operator wrapper to pick the serialization method

Options 1 and 3 are attractive because they are simple.  We could also use the ~#ifdef HAVE_MPI ... #else ... #endif~ approach with Option 1 to set a typedef:
#+BEGIN_SRC C++
template<class T>
class MpiOpTraitsTypedef {
public:
#ifdef HAVE_MPI
  typedef MpiOpTraits<T, MpiSerializationTraits<T> > op_traits_type;
#else
  typedef MpiOpTraits<T, NoMpiSerializationTraits<T> > op_traits_type;
#endif // HAVE_MPI
};
#+END_SRC
(This neglects the fact that ~MPI_Op~ is not defined if not building with MPI.  We would need to wrap ~MPI_Op~ in this case.)

Option 2 is attractive because it allows run-time choice of serialization method.  However, it presents some difficulties because one must associate additional data with the ~MPI_Op~, namely, the ~Serializer~ instance.  MPI does not directly allow supplying additional data to the reduction besides the actual input and output buffers.  It also does not implement the "caching" facility (see Section 6.7 of the MPI 3.0 Standard) for ~MPI_Op~, so we cannot attach the additional data to the ~MPI_Op~ itself.  
1. We could attach the data to the ~MPI_Datatype~ argument.  We would then require this always to be ~MPI_PACKED~ whenever we need serialization.  This works if the whole application only uses one serialization method at a time.  If the application is calling MPI directly, outside of our wrappers, then this could cause problems if it also tries to attach data to ~MPI_PACKED~.
2. We could use hidden global data: a pointer to the polymorphic ~Serializer~ instance.  This would prevent users from running in the equivalent of ~MPI_THREAD_MULTIPLE~ mode with our interface; they would have to serialize access to reductions among threads in a process, and introduce locking or atomic updates to protect access to the global data.
3. We could pack additional information in the beginning of the buffers that tells us which serializer to use.  (Thus, subclasses of ~Serializer~ must themselves be serializable, via a simple universal method.)  Then, we would construct the ~Serializer~ on the fly.

#~MPI_Op_commutative~ lets you query an ~MPI_Op~ for its commutativity (MPI 3.0 Standard, Section 5.9).



