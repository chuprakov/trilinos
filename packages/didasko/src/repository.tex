% @HEADER
% ***********************************************************************
% 
%            Trilinos: An Object-Oriented Solver Framework
%                 Copyright (2001) Sandia Corporation
% 
% Under terms of Contract DE-AC04-94AL85000, there is a non-exclusive
% license for use of this work by or on behalf of the U.S. Government.
% 
% This library is free software; you can redistribute it and/or modify
% it under the terms of the GNU Lesser General Public License as
% published by the Free Software Foundation; either version 2.1 of the
% License, or (at your option) any later version.
%  
% This library is distributed in the hope that it will be useful, but
% WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
% Lesser General Public License for more details.
%  
% You should have received a copy of the GNU Lesser General Public
% License along with this library; if not, write to the Free Software
% Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307
% USA
% Questions? Contact Michael A. Heroux (maherou@sandia.gov) 
% 
% ***********************************************************************
% @HEADER

\section{Overview of Epetra}

Epetra provides the fundamental construction
routines and services function that are required for serial and
parallel linear algebra libraries.  Epetra provides the underlying
foundation for all Trilinos solvers.

Epetra contains a number of classes.  They can be categorized as follows:
\begin{itemize}
\item Primary parallel user classes.  These are typically the most
  important classes for most users.
\begin{enumerate}
\item Communicator class: Epetra\_Comm - Contains specific information
  about the parallel machine we are using.  Currently supports serial,
  MPI and prototype hybrid MPI/threaded parallel programming models.
\item Map classes: Epetra\_Map, Epetra\_LocalMap, Epetra\_BlockMap -
  Contain information used to distribute vectors, matrices and other
  objects on a parallel (or serial) machine.
\item Vector class: Epetra\_Vector - Real double precision vector class.
  Supports construction and use of vectors on a parallel machine.
\item Multi-vector class: Epetra\_MultiVector - Real double precision
  multi-vector class.  Supports construction and use of multi-vectors on
  a parallel machine.  A multi-vector is a collection vectors.  It is a
  generalization of a 2D array.
\item Sparse row graph class: Epetra\_CrsGraph - Allows construction of a
  serial or parallel graph.  The graph determines the communication
  pattern for subsequent matrix objects.
\item Pure virtual row matrix class: Epetra\_RowMatrix - Pure virtual
  class that specifies interfaces needed to do most of the common
  operations required by a row matrix.  The Epetra\_LinearProblem expects
  the matrix as a Epetra\_RowMatrix.  Both the Epetra\_CrsMatrix and
  Epetra\_VbrMatrix classes implement the Epetra\_RowMatrix interface and
  therefore objects from either of these classes can be used with
  Epetra\_LinearProblem and with AztecOO.  Furthermore, any class that
  implements Epetra\_RowMatrix can be used with Epetra\_LinearProblem and
  AztecOO.
\item Sparse row matrix class: Epetra\_CrsMatrix - Real double precision
  sparse matrix class.  Supports construction and use of row-wise sparse
  matrices.
\item Sparse block row matrix class: Epetra\_VbrMatrix - Real double
  precision block sparse matrix class.  Supports construction and use of
  row-wise block sparse matrices.
\item Import/Export classes: Epetra\_Import and Epetra\_Export -
  Constructed from two Epetra\_BlockMap (or Epetra\_Map or
  Epetra\_LocalMap).  Allows efficient transfer of objects built using
  one map to a new object with a new map.  Supports local and global
  permutations, overlapping Schwarz operations and many other data
  movement algorithms.
\end{enumerate}     

\item Primary serial user classes.  These classes provide object
  oriented interfaces to LAPACK capabilities, providing easy access to
  the most powerful numerical methods in LAPACK.
\begin{enumerate}
\item General dense matrix/vector classes: Epetra\_SerialDenseMatrix,
  Epetra\_SerialDenseVector - Lightweight dense matrix and vector classes
  used to define matrices, left-hand-sides and right-hand-sides for the
  serial solver classes.
\item General dense solver class: Epetra\_SerialDenseSolver - Provides
  dense matrix services such as factorizations, solves, QR, SVD, etc.,
  with special attention focused on numerically robust solutions.
\item Symmetric dense matrix class: Epetra\_SerialSymDenseMatrix -
  Similar to Epetra\_SerialDenseMatrix except focused specifically on
  symmetric matrices.
\item Symmetric definite dense solver: Epetra\_SerialSpdDenseSolver -
  Similar to Epetra\_SerialDenseSolver except focused specifically on
  symmetric definite systems.
\end{enumerate}

\item Utility classes. 

\begin{enumerate}
\item Timing class: Epetra\_Time - Provides timing functions for the
  purposes of performance analysis.
  
\item Floating point operation class: Epetra\_Flops - Provides floating
  point operations (FLOPS) counting and reporting functions for the
  purposes of performance analysis.  All Epetra computational classes
  accumulate FLOP counts associated with the {\sl this} object of the
  computations.
\item Distributed directory class: Epetra\_Directory - Allows
  construction of a distributed directory.  Once constructed, a
  directory allows one to access randomly distributed objects in an
  efficient, scalable manner.  This class is intended for support of
  general Epetra\_BlockMap and Epetra\_Map objects, but is useful in other
  settings as well.
\item BLAS wrapper class: Epetra\_BLAS - A ``thin'' layer of C++ code
  wrapping the basic linear algebra subprograms (BLAS).  This class
  provides a single instance interface between Epetra and the BLAS.  In
  this way we can easily switch BLAS interfaces and manage the
  C++/Fortran translation differences that exist between different
  computer systems.  This class also provides a very convenient way to
  templatize the BLAS.
\item LAPACK wrapper class: Epetra\_LAPACK - A ``thin'' layer of C++ code
  wrapping LAPACK.  Like Epetra\_BLAS, it provides nice C++ access to
  LAPACK.
\end{enumerate}

\end{itemize}


























%%%
%%%
%%%

\section{A First Program Using Trilinos}

We now report a complete code, which creates some distribute vectors,
fills them, and finally computes their norms. This is done in the
following program (see \verb!${TRILINOS_HOME}/Tutorials/epetra/ex1.c!.
\begin{verbatim}
 1: #include <iostream>
 2: #include "Epetra_config.h"
 3: #ifdef HAVE_MPI
 4: #include "mpi.h"
 5: #include "Epetra_MpiComm.h"
 6: #else
 7: #include "Epetra_SerialComm.h"
 8: #endif
 9: #include "Epetra_Map.h"
10: #include "Epetra_Vector.h"
11:
12: int main(int argc, char *argv[])
13: {
14: 
15: #ifdef HAVE_MPI
16:   MPI_Init(&argc, &argv);
17:   Epetra_MpiComm Comm(MPI_COMM_WORLD);
18: #else
19:   Epetra_SerialComm Comm;
20: #endif
  
22:   // communicator info
23:   int MyPID = Comm.MyPID();
24:   int NumProc = Comm.NumProc();
25:   cout << Comm << endl;

26:   bool verbose = (MyPID==0);

28:  // Total number of elements in the vector
29:  int NumElements = 5;

30:  // Construct a Map with NumElements and index base of 0
31:  Epetra_Map Map(NumElements, 0, Comm);

33:  // Create x vector
34:  Epetra_Vector x(Map);

36:  // set all the elements to a scalar value
37:  x.PutScalar( 1.0 );
38:  if( verbose ) cout << "Expecting a vector of 1's\n";
39:  cout << x;

41:  // random numbers
42:  x.Random();
43:  if( verbose) cout << "Expecting a vector of random numbers\n";
44:  cout << x;

46:  // compute few norms
47:  double xnorm1, xnorm2, xnorminf;
48:  x.Norm1(&xnorm1);
49:  x.Norm2(&xnorm2);
50:  x.NormInf(&xnorminf);

52:  if( verbose ) {
53:    cout << "  1 norm of x (random numbers) = " << xnorm1 << endl;
54:    cout << "  2 norm of x (random numbers) = " << xnorm2 << endl;
55:    cout << "Inf norm of x (random numbers) = " << xnorminf << endl;
56:  }
57:  
58: #ifdef HAVE_MPI
59:  MPI_Finalize();
60: #endif
61:
62:  return( EXIT_SUCCESS );
63:  
64: }
\end{verbatim}

To compile this program under LINUX with MPI, the \verb!Makefile! will
look approximately like this:
\begin{verbatim}
TRILINOS_HOME = $(HOME)/Trilinos/
TRILINOS_LIB  = $(HOME)/Trilinos/LINUX_MPI

include $(TRILINOS_HOME)/build/makefile.LINUX.MPI

MY_COMPILER_FLAGS = -DHAVE_CONFIG_H $(CXXFLAGS) -c \
                    -I$(TRILINOS_LIB)/include/
                    
MY_LINKER_FLAGS = $(LDFLAGS) $(TEST_C_OBJ) \
        -L$(TRILINOS_LIB)lib \
        -lepetra $(LIB_PATHS) $(ARCH_LIBS) -lblas

ex1: ex1.cpp
        $(CXX)    ex1.cpp $(MY_COMPILER_FLAGS) 
        $(LINKER) ex1.o   $(MY_LINKER_FLAGS)    -o ex1.exe
\end{verbatim}

(Note that GNU \verb!make! must be used in order to has this
\verb!Makefile! work.)

\begin{remark}
  Although some C and FORTRAN interfaces are provided, the best way to
  exploit the full potentiality of Trilinos, is to use C++. For this
  reason, this document will report C++ only.
\end{remark}

Lines 1-10 include the required file. All the programs using Epetra must
have the \verb!#include "Epetra_config.h"! line, as this include file
contains all the definitions provided by {\tt configure}. \verb!HAVE_MPI!
is the variables used in Epetra to distinguish between sequential and
parallel run. Note that, in the code, \verb!HAVE_MPI! is used here, and
only in lines 15-20 (to initialize MPI) and in lines 58-60 (to finalize
MPI). All the other communication issues are hidden to the user, and
internally handled by Epetra. 

Once a map has been created, we can create vectors (and matrices) based
on this map. This is done in line 34. The elements of {\tt x} are filled
in line 37 (constant value), or in line 42 (for random numbers).

In lines 48-50 we compute the 1-norm, 2-norm, and $\infty-$norm
of {\tt x}.

Finally, lines 58-62 finalize MPI (if \verb!HAVE_MPI! is defined) and
quit.

